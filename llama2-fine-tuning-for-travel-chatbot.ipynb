{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:14.242604Z","iopub.execute_input":"2025-05-24T01:53:14.242867Z","iopub.status.idle":"2025-05-24T01:53:14.247874Z","shell.execute_reply.started":"2025-05-24T01:53:14.242844Z","shell.execute_reply":"2025-05-24T01:53:14.247086Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"%%capture\n%pip install accelerate peft bitsandbytes transformers trl\n%pip install -U flash-attn\n%pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:14.249044Z","iopub.execute_input":"2025-05-24T01:53:14.249328Z","iopub.status.idle":"2025-05-24T01:53:24.239843Z","shell.execute_reply.started":"2025-05-24T01:53:14.249311Z","shell.execute_reply":"2025-05-24T01:53:24.239120Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"hf_token = \"hf_token\"\nfrom huggingface_hub import login\n\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:24.240943Z","iopub.execute_input":"2025-05-24T01:53:24.241266Z","iopub.status.idle":"2025-05-24T01:53:24.331584Z","shell.execute_reply.started":"2025-05-24T01:53:24.241234Z","shell.execute_reply":"2025-05-24T01:53:24.330824Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import Dataset\n\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:37:56.292078Z","iopub.execute_input":"2025-05-24T05:37:56.292566Z","iopub.status.idle":"2025-05-24T05:37:56.297195Z","shell.execute_reply.started":"2025-05-24T05:37:56.292545Z","shell.execute_reply":"2025-05-24T05:37:56.296340Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n# Model from Hugging Face hub\nbase_model = \"meta-llama/Llama-2-7b-chat-hf\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:24.339039Z","iopub.execute_input":"2025-05-24T01:53:24.339948Z","iopub.status.idle":"2025-05-24T01:53:24.354330Z","shell.execute_reply.started":"2025-05-24T01:53:24.339920Z","shell.execute_reply":"2025-05-24T01:53:24.353635Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Running Simple Llama","metadata":{}},{"cell_type":"code","source":"model_tokenizer = AutoTokenizer.from_pretrained(base_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:24.355356Z","iopub.execute_input":"2025-05-24T01:53:24.355666Z","iopub.status.idle":"2025-05-24T01:53:25.210975Z","shell.execute_reply.started":"2025-05-24T01:53:24.355640Z","shell.execute_reply":"2025-05-24T01:53:25.210420Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"inputs = model_tokenizer(\"I live in Sydney NSW in Australia, tell me the places near sydney but not inside sydney where I can go for a day trip using public transport\"\n                  , return_tensors=\"pt\"\n                  ).to(device)\ninputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:25.211939Z","iopub.execute_input":"2025-05-24T01:53:25.212184Z","iopub.status.idle":"2025-05-24T01:53:25.220761Z","shell.execute_reply.started":"2025-05-24T01:53:25.212162Z","shell.execute_reply":"2025-05-24T01:53:25.219862Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    1,   306,  5735,   297, 16198,  3865, 29956,   297,  8314, 29892,\n          2649,   592,   278,  7600,  2978, 19205,  3801,   541,   451,  2768,\n         19205,  3801,   988,   306,   508,   748,   363,   263,  2462, 17487,\n           773,   970,  8608]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# model = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.float16, device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:25.221488Z","iopub.execute_input":"2025-05-24T01:53:25.221687Z","iopub.status.idle":"2025-05-24T01:53:25.237512Z","shell.execute_reply.started":"2025-05-24T01:53:25.221672Z","shell.execute_reply":"2025-05-24T01:53:25.236848Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# with torch.no_grad():\n#     output_ids = model.generate(**inputs, top_k=10, do_sample=True)\n# # output_ids[0] is logits\n# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n# print(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:25.238188Z","iopub.execute_input":"2025-05-24T01:53:25.238349Z","iopub.status.idle":"2025-05-24T01:53:25.251681Z","shell.execute_reply.started":"2025-05-24T01:53:25.238335Z","shell.execute_reply":"2025-05-24T01:53:25.251178Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"ds1 = load_dataset(\"JasleenSingh91/travel-questions-response\")\ndf1 = pd.DataFrame(ds1['train'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:25.252306Z","iopub.execute_input":"2025-05-24T01:53:25.252506Z","iopub.status.idle":"2025-05-24T01:53:26.334723Z","shell.execute_reply.started":"2025-05-24T01:53:25.252491Z","shell.execute_reply":"2025-05-24T01:53:26.334176Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"x = 4897\nprint(df1.iloc[x][\"Input\"])\nprint(\"----------------------------------\\n\")\nprint(df1.iloc[x][\"Response_1\"])\nprint(\"----------------------------------\\n\")\nprint(df1.iloc[x][\"Response_2\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.335443Z","iopub.execute_input":"2025-05-24T01:53:26.335669Z","iopub.status.idle":"2025-05-24T01:53:26.341121Z","shell.execute_reply.started":"2025-05-24T01:53:26.335650Z","shell.execute_reply":"2025-05-24T01:53:26.340156Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Recommend solo adventure destinations in Africa, including safaris, hiking to volcanoes, and exploring ancient cultures.\n----------------------------------\n\nAbsolutely! Africa is a wonderful continent for solo adventures, offering a diverse range of experiences from adventure safaris, mountain treks, to exploring ancient cultures. Here are some recommendations:\n1. Serengeti National Park, Tanzania: Known as the \"Greatest Wildlife Show on Earth,\" the Serengeti is a must-visit destination for solo travelers who love wildlife. Go on a safari to witness the annual Great Migration, where millions of animals migrate in search of water and fresh grazing lands. Stay at a tented camp or lodge for an authentic African experience.\n2. Mount Kilimanjaro, Tanzania: This is a once-in-a-lifetime adventure for solo travelers seeking a challenge. Mount Kilimanjaro is the highest peak in Africa and offers breathtaking views of the African landscape. Join a guided tour, hire a guide and porters, and tackle the summit.\n3. Gorilla Trekking in Rwanda or Uganda: For solo travelers interested in nature and wildlife, gorilla trekking is an unforgettable experience. Rwanda and Uganda offer affordable gorilla permits and guided tours to help you track these endangered primates.\n4. Petra, Jordan: Discover the ancient city of Petra, a UNESCO World Heritage Site. Explore the famous Treasury, the Monastery, and other ruins on foot. Petra is a popular tourist destination, but it can be easily explored solo with the help of a local guide.\n5. Marrakech, Morocco: Marrakech offers a rich cultural experience with its bustling markets, delicious food, and historic sites. Explore the Bahia Palace, the Jardin Majorelle, and the vibrant Jemaa El-Fnaa square. Marrakech\n----------------------------------\n\n Hello there! As a travel agent, I'd be delighted to recommend some exciting solo adventure destinations in Africa that cater to your interests. From safaris and volcano hiking to exploring ancient cultures, Africa has plenty to offer. Here are some unique experiences you might enjoy:\n\n1. Safaris in Maasai Mara, Kenya:\nMaasai Mara National Reserve is renowned for its abundant wildlife and the Mara River, which attracts countless hippos, crocodiles, and birds. Experience a thrilling safari adventure with expert guides who will give you an in-depth look into the lives of Africa's iconic species. Start your day with a hot air balloon ride over the savannah for breathtaking views. For a more immersive experience, choose a fly-and-camp safari package, allowing you to spend a night under the stars with your guides.\n\nCost: $100-$500 per person per day\nDuration: 3-5 days\nAccommodation: Luxury safari camps or mid-range lodges\n\n2. Hiking Mount Nyiragongo, Democratic Republic of Congo:\nMount Nyiragongo is a majestic volcano with a glowing lava lake at its base. Hike the 11-kilometer trail to the summit to witness the mesmerizing views and unique geological formations. The hike takes around 2-3 days and is considered moderate to challenging. Be prepared for stunning views of the surrounding landscape and the opportunity to spot unique flora and fauna.\n\nCost: $500-$1,000 per person\nDuration: 2-3 days\nAccommodation: Basic camping facilities or basic lodges\n\n3. Discovery of Ancient Cultures in Ethiopia:\nExplore the ancient rock-hewn churches of Lalibela, a UNESCO World Heritage Site. The churches, built in the 12th century, are adorned with intricate carvings and mosaics. Visit the nearby Mount Senkelle Wildlife Sanctuary to see the elusive Gelada baboons. In the evenings, enjoy local cuisine and drink coffee with the\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"df1.iloc[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.341829Z","iopub.execute_input":"2025-05-24T01:53:26.342123Z","iopub.status.idle":"2025-05-24T01:53:26.359469Z","shell.execute_reply.started":"2025-05-24T01:53:26.342089Z","shell.execute_reply":"2025-05-24T01:53:26.358918Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"Input         What activities and places should I explore in...\nResponse_1    I'd be happy to help you plan an exciting and ...\nResponse_2     Ahaha, I'd be thrilled to help you plan your ...\nName: 1, dtype: object"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# making the system prompt text\nsystem_prompt = \"Can you please help me with.\"\n\nclass MyDataset(Dataset):\n    def __init__(self, dataframe):\n        self.df = dataframe\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        system_msg = \"Please help me with \"\n        user_input = self.df.iloc[index][\"Input\"]\n        assistant_response = self.df.iloc[index][\"Response_1\"]\n        return f\"<s>[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n{user_input} [/INST]{assistant_response} </s>\"\n\ndef format_row(user_input, assistant_response, system_msg=system_prompt):\n    return f\"<s>[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n{user_input} [/INST]{assistant_response} </s>\"\n\ndf1[\"text\"] = df1.apply(lambda row: format_row(row[\"Input\"], row[\"Response_1\"]), axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.360181Z","iopub.execute_input":"2025-05-24T01:53:26.360364Z","iopub.status.idle":"2025-05-24T01:53:26.421658Z","shell.execute_reply.started":"2025-05-24T01:53:26.360350Z","shell.execute_reply":"2025-05-24T01:53:26.420934Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.424452Z","iopub.execute_input":"2025-05-24T01:53:26.424721Z","iopub.status.idle":"2025-05-24T01:53:26.432812Z","shell.execute_reply.started":"2025-05-24T01:53:26.424703Z","shell.execute_reply":"2025-05-24T01:53:26.432121Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"                                               Input  \\\n0  Can you suggest the best travel destinations f...   \n1  What activities and places should I explore in...   \n2  Can you recommend relaxing destinations to vis...   \n3            What are the top winter vacation spots?   \n4  Can you recommend a great vacation destination...   \n\n                                          Response_1  \\\n0   Absolutely! To help me suggest the best trave...   \n1  I'd be happy to help you plan an exciting and ...   \n2   Absolutely! Winter can be a beautiful and pea...   \n3  Absolutely! I'd be happy to help you plan your...   \n4   Absolutely! Based on your preferences, I woul...   \n\n                                          Response_2  \\\n0   Hi there! I'd be delighted to help you plan y...   \n1   Ahaha, I'd be thrilled to help you plan your ...   \n2   As a travel agent, I'd be delighted to recomm...   \n3   As a travel agent, I'm happy to share with yo...   \n4   I'd love to help you plan an unforgettable va...   \n\n                                                text  \n0  <s>[INST] <<SYS>>\\nCan you please help me with...  \n1  <s>[INST] <<SYS>>\\nCan you please help me with...  \n2  <s>[INST] <<SYS>>\\nCan you please help me with...  \n3  <s>[INST] <<SYS>>\\nCan you please help me with...  \n4  <s>[INST] <<SYS>>\\nCan you please help me with...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Input</th>\n      <th>Response_1</th>\n      <th>Response_2</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Can you suggest the best travel destinations f...</td>\n      <td>Absolutely! To help me suggest the best trave...</td>\n      <td>Hi there! I'd be delighted to help you plan y...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What activities and places should I explore in...</td>\n      <td>I'd be happy to help you plan an exciting and ...</td>\n      <td>Ahaha, I'd be thrilled to help you plan your ...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Can you recommend relaxing destinations to vis...</td>\n      <td>Absolutely! Winter can be a beautiful and pea...</td>\n      <td>As a travel agent, I'd be delighted to recomm...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What are the top winter vacation spots?</td>\n      <td>Absolutely! I'd be happy to help you plan your...</td>\n      <td>As a travel agent, I'm happy to share with yo...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Can you recommend a great vacation destination...</td>\n      <td>Absolutely! Based on your preferences, I woul...</td>\n      <td>I'd love to help you plan an unforgettable va...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"train_set, eval_set = train_test_split(df1, train_size=0.8, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.433519Z","iopub.execute_input":"2025-05-24T01:53:26.433724Z","iopub.status.idle":"2025-05-24T01:53:26.451860Z","shell.execute_reply.started":"2025-05-24T01:53:26.433708Z","shell.execute_reply":"2025-05-24T01:53:26.451300Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"print(train_set.shape)\nprint(eval_set.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.452564Z","iopub.execute_input":"2025-05-24T01:53:26.452762Z","iopub.status.idle":"2025-05-24T01:53:26.467407Z","shell.execute_reply.started":"2025-05-24T01:53:26.452746Z","shell.execute_reply":"2025-05-24T01:53:26.466665Z"}},"outputs":[{"name":"stdout","text":"(4807, 4)\n(1202, 4)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"train_set","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.468132Z","iopub.execute_input":"2025-05-24T01:53:26.468577Z","iopub.status.idle":"2025-05-24T01:53:26.486160Z","shell.execute_reply.started":"2025-05-24T01:53:26.468550Z","shell.execute_reply":"2025-05-24T01:53:26.485605Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"                                                  Input  \\\n5634  Suggest a winter hiking destination with stunn...   \n4434  Suggest a budget-friendly mountain Airbnb with...   \n2350  Recommend budget-friendly mountain resorts for...   \n5414  Recommend solo travel destinations for explori...   \n3303  Plan a fall trip to a destination known for it...   \n...                                                 ...   \n3772  Recommend travel destinations offering both re...   \n5191  Recommend solo travel destinations in India of...   \n5226  Suggest the best family vacation spots in Ital...   \n5390  Recommend solo travel destinations in Greece f...   \n860   What are the best fall destinations for vibran...   \n\n                                             Response_1  \\\n5634   I'd recommend Lake Tahoe in California and Ne...   \n4434  Instruction: I'd be happy to help you find a b...   \n2350   I'd be happy to help you find some budget-fri...   \n5414  Subject: Exciting Solo Cave Exploring Adventur...   \n3303  Destination: Vermont, USA\\n\\nOverview:\\nVermon...   \n...                                                 ...   \n3772  Absolutely! I'd be happy to help you plan a va...   \n5191  Absolutely, I'd be happy to help you plan a sp...   \n5226  Italy is a wonderful destination for families ...   \n5390  Absolutely, I'd be happy to help you plan a so...   \n860    Absolutely, I'd be happy to help you plan a f...   \n\n                                             Response_2  \\\n5634   A fantastic winter hiking destination with st...   \n4434   Ah, I've got just the thing for you!\\n\\nI rec...   \n2350   Ahaha, I'm excited to help you plan your next...   \n5414   Ah, an adventure-seeking solo traveler, eh? N...   \n3303   Fall is indeed a wonderful time to travel, wi...   \n...                                                 ...   \n3772   As a travel agent, I'm excited to recommend t...   \n5191   Ah, thank you for considering India, a countr...   \n5226   Italy - the land of love, art, history, and d...   \n5390   Welcome! As a travel agent, I'm thrilled to s...   \n860    Ah, autumn is one of my favorite seasons! I'v...   \n\n                                                   text  \n5634  <s>[INST] <<SYS>>\\nCan you please help me with...  \n4434  <s>[INST] <<SYS>>\\nCan you please help me with...  \n2350  <s>[INST] <<SYS>>\\nCan you please help me with...  \n5414  <s>[INST] <<SYS>>\\nCan you please help me with...  \n3303  <s>[INST] <<SYS>>\\nCan you please help me with...  \n...                                                 ...  \n3772  <s>[INST] <<SYS>>\\nCan you please help me with...  \n5191  <s>[INST] <<SYS>>\\nCan you please help me with...  \n5226  <s>[INST] <<SYS>>\\nCan you please help me with...  \n5390  <s>[INST] <<SYS>>\\nCan you please help me with...  \n860   <s>[INST] <<SYS>>\\nCan you please help me with...  \n\n[4807 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Input</th>\n      <th>Response_1</th>\n      <th>Response_2</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5634</th>\n      <td>Suggest a winter hiking destination with stunn...</td>\n      <td>I'd recommend Lake Tahoe in California and Ne...</td>\n      <td>A fantastic winter hiking destination with st...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>4434</th>\n      <td>Suggest a budget-friendly mountain Airbnb with...</td>\n      <td>Instruction: I'd be happy to help you find a b...</td>\n      <td>Ah, I've got just the thing for you!\\n\\nI rec...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>2350</th>\n      <td>Recommend budget-friendly mountain resorts for...</td>\n      <td>I'd be happy to help you find some budget-fri...</td>\n      <td>Ahaha, I'm excited to help you plan your next...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>5414</th>\n      <td>Recommend solo travel destinations for explori...</td>\n      <td>Subject: Exciting Solo Cave Exploring Adventur...</td>\n      <td>Ah, an adventure-seeking solo traveler, eh? N...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>3303</th>\n      <td>Plan a fall trip to a destination known for it...</td>\n      <td>Destination: Vermont, USA\\n\\nOverview:\\nVermon...</td>\n      <td>Fall is indeed a wonderful time to travel, wi...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3772</th>\n      <td>Recommend travel destinations offering both re...</td>\n      <td>Absolutely! I'd be happy to help you plan a va...</td>\n      <td>As a travel agent, I'm excited to recommend t...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>5191</th>\n      <td>Recommend solo travel destinations in India of...</td>\n      <td>Absolutely, I'd be happy to help you plan a sp...</td>\n      <td>Ah, thank you for considering India, a countr...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>5226</th>\n      <td>Suggest the best family vacation spots in Ital...</td>\n      <td>Italy is a wonderful destination for families ...</td>\n      <td>Italy - the land of love, art, history, and d...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>Recommend solo travel destinations in Greece f...</td>\n      <td>Absolutely, I'd be happy to help you plan a so...</td>\n      <td>Welcome! As a travel agent, I'm thrilled to s...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>What are the best fall destinations for vibran...</td>\n      <td>Absolutely, I'd be happy to help you plan a f...</td>\n      <td>Ah, autumn is one of my favorite seasons! I'v...</td>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nCan you please help me with...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4807 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# train_set = MyDataset(train_set)\n# eval_set = MyDataset(eval_set)\n\ntrain_set = Dataset.from_pandas(train_set[[\"text\"]])\neval_set = Dataset.from_pandas(eval_set[[\"text\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.486777Z","iopub.execute_input":"2025-05-24T01:53:26.487089Z","iopub.status.idle":"2025-05-24T01:53:26.580130Z","shell.execute_reply.started":"2025-05-24T01:53:26.487072Z","shell.execute_reply":"2025-05-24T01:53:26.579550Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"train_set[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.580808Z","iopub.execute_input":"2025-05-24T01:53:26.581035Z","iopub.status.idle":"2025-05-24T01:53:26.586024Z","shell.execute_reply.started":"2025-05-24T01:53:26.581018Z","shell.execute_reply":"2025-05-24T01:53:26.585301Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"{'text': \"<s>[INST] <<SYS>>\\nCan you please help me with.\\n<</SYS>>\\n\\nSuggest a winter hiking destination with stunning landscapes and challenging trails. [/INST] I'd recommend Lake Tahoe in California and Nevada as a winter hiking destination. Known for its breathtaking landscapes, Lake Tahoe offers some of the most challenging and rewarding hikes during the winter months.\\n\\nPopular Trails:\\n1. Cascade Falls Trail: A moderately difficult trail, featuring a stunning frozen waterfall and beautiful views of Lake Tahoe. (Approx. 5 Miles Roundtrip)\\n2. Eagle Falls Trail: This challenging hike to Eagle Falls requires some ice axe and crampons usage, making it perfect for experienced hikers looking for a true winter adventure. (Approx. 5 Miles Roundtrip)\\n3. Gladys Lake Trail: The trail leads to a picturesque lake that's surrounded by snowy peaks and frozen trees. Though the trail is challenging, the reward is worth the effort. (Approx. 7 Miles Roundtrip)\\n\\nWeather Conditions: Winter temperatures typically range from 20°F (-6°C) to 40°F (4°C). Be prepared for snow and ice on the trails and pack appropriate hiking gear, including microspikes or crampons, a pair of gaiters, and trekking poles.\\n\\nAdditional Tips:\\n1. Always check weather conditions and road closures before embarking on your hike.\\n2. Bring plenty of water and high-energy snacks for the journey.\\n3. Be aware of avalanche risks and potential hazards and stay informed about the location and status of any known hazards.\\n4. Let someone know your hiking plans and intended route before you leave, along with an expected return time.\\n5. Dress in layers to help regulate your body temperature while hiking.\\n6. Carry a map, compass, and GPS, and </s>\",\n '__index_level_0__': 5634}"},"metadata":{}}],"execution_count":45},{"cell_type":"markdown","source":"# Model setup with LoRa 4 bit quantization","metadata":{}},{"cell_type":"code","source":"model=None\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quant_config,\n    attn_implementation=\"flash_attention_2\",\n    device_map={\"\": 0}\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:53:26.586811Z","iopub.execute_input":"2025-05-24T01:53:26.587102Z","iopub.status.idle":"2025-05-24T01:54:32.134591Z","shell.execute_reply.started":"2025-05-24T01:53:26.587079Z","shell.execute_reply":"2025-05-24T01:54:32.134056Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f1b3375edbd47889aa8bbf73eddd1fd"}},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:54:32.135319Z","iopub.execute_input":"2025-05-24T01:54:32.135570Z","iopub.status.idle":"2025-05-24T01:54:32.143794Z","shell.execute_reply.started":"2025-05-24T01:54:32.135552Z","shell.execute_reply":"2025-05-24T01:54:32.143200Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"# Training Config","metadata":{}},{"cell_type":"code","source":"peft_params = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\ntraining_params = SFTConfig(\n    output_dir=\"/kaggle/working/output\",\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=8,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    max_length = 500,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:27:59.924051Z","iopub.execute_input":"2025-05-24T08:27:59.924633Z","iopub.status.idle":"2025-05-24T08:27:59.977254Z","shell.execute_reply.started":"2025-05-24T08:27:59.924611Z","shell.execute_reply":"2025-05-24T08:27:59.976497Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_set,\n    eval_dataset=eval_set,\n    peft_config=peft_params,\n    args=training_params,\n    # max_length =1000,\n    # packing=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:28:00.126150Z","iopub.execute_input":"2025-05-24T08:28:00.126375Z","iopub.status.idle":"2025-05-24T08:28:08.866436Z","shell.execute_reply.started":"2025-05-24T08:28:00.126357Z","shell.execute_reply":"2025-05-24T08:28:08.865860Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/4807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74e3c29ea63e4cbcb5b25eb6b56c00ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/4807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ced49978acb54c8e848f709a575c344d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/4807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e11cfc5a31674f2b852a7ddbb3e1edd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/4807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d5256061674819a06f11bf999e3148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/1202 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca986d7edd384462a1bfa08ca345e10e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/1202 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1947c4dcd3d48769111efef7f711991"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/1202 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e69b5fa68843df84c5c51efb3a64a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/1202 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd6c03c64f214a7eadf666c1630950c1"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":91},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\nprint(torch.cuda.memory_summary())\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T01:54:40.748276Z","iopub.execute_input":"2025-05-24T01:54:40.748540Z","iopub.status.idle":"2025-05-24T05:14:26.612125Z","shell.execute_reply.started":"2025-05-24T01:54:40.748516Z","shell.execute_reply":"2025-05-24T05:14:26.611546Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |  11995 MiB |  12117 MiB |  33272 GiB |  33261 GiB |\n|       from large pool |  11351 MiB |  11601 MiB |  33241 GiB |  33230 GiB |\n|       from small pool |    644 MiB |    644 MiB |     30 GiB |     30 GiB |\n|---------------------------------------------------------------------------|\n| Active memory         |  11995 MiB |  12117 MiB |  33272 GiB |  33261 GiB |\n|       from large pool |  11351 MiB |  11601 MiB |  33241 GiB |  33230 GiB |\n|       from small pool |    644 MiB |    644 MiB |     30 GiB |     30 GiB |\n|---------------------------------------------------------------------------|\n| Requested memory      |  11965 MiB |  12087 MiB |  33271 GiB |  33259 GiB |\n|       from large pool |  11321 MiB |  11571 MiB |  33240 GiB |  33229 GiB |\n|       from small pool |    644 MiB |    644 MiB |     30 GiB |     30 GiB |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |  13090 MiB |  14560 MiB |  15052 MiB |   1962 MiB |\n|       from large pool |  12442 MiB |  13912 MiB |  14402 MiB |   1960 MiB |\n|       from small pool |    648 MiB |    648 MiB |    650 MiB |      2 MiB |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |   1094 MiB |   6179 MiB |  22481 GiB |  22480 GiB |\n|       from large pool |   1090 MiB |   6178 MiB |  22447 GiB |  22446 GiB |\n|       from small pool |      3 MiB |      6 MiB |     33 GiB |     33 GiB |\n|---------------------------------------------------------------------------|\n| Allocations           |    1965    |    1965    |  581105    |  579140    |\n|       from large pool |     681    |     683    |  482922    |  482241    |\n|       from small pool |    1284    |    1284    |   98183    |   96899    |\n|---------------------------------------------------------------------------|\n| Active allocs         |    1965    |    1965    |  581105    |  579140    |\n|       from large pool |     681    |     683    |  482922    |  482241    |\n|       from small pool |    1284    |    1284    |   98183    |   96899    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |     356    |     359    |     361    |       5    |\n|       from large pool |      32    |      35    |      36    |       4    |\n|       from small pool |     324    |     324    |     325    |       1    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |     162    |     163    |  281767    |  281605    |\n|       from large pool |      94    |      95    |  253119    |  253025    |\n|       from small pool |      68    |      83    |   28648    |   28580    |\n|---------------------------------------------------------------------------|\n| Oversize allocations  |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\n|===========================================================================|\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 3:16:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.137200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.832400</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.777100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=75, training_loss=0.9155764770507813, metrics={'train_runtime': 11983.5164, 'train_samples_per_second': 0.401, 'train_steps_per_second': 0.006, 'total_flos': 8.560893240764006e+16, 'train_loss': 0.9155764770507813})"},"metadata":{}}],"execution_count":50},{"cell_type":"markdown","source":"## Save Model and tokenizer","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/output\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:14:26.612837Z","iopub.execute_input":"2025-05-24T05:14:26.613094Z","iopub.status.idle":"2025-05-24T05:14:39.202233Z","shell.execute_reply.started":"2025-05-24T05:14:26.613077Z","shell.execute_reply":"2025-05-24T05:14:39.201420Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"model_tokenizer.save_pretrained(\"/kaggle/working/tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:36:01.359291Z","iopub.execute_input":"2025-05-24T05:36:01.359571Z","iopub.status.idle":"2025-05-24T05:36:01.416096Z","shell.execute_reply.started":"2025-05-24T05:36:01.359550Z","shell.execute_reply":"2025-05-24T05:36:01.415197Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/tokenizer/tokenizer_config.json',\n '/kaggle/working/tokenizer/special_tokens_map.json',\n '/kaggle/working/tokenizer/tokenizer.model',\n '/kaggle/working/tokenizer/added_tokens.json',\n '/kaggle/working/tokenizer/tokenizer.json')"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"# !mkdir output\n# !mkdir tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:35:26.243768Z","iopub.execute_input":"2025-05-24T05:35:26.244551Z","iopub.status.idle":"2025-05-24T05:35:26.641099Z","shell.execute_reply.started":"2025-05-24T05:35:26.244525Z","shell.execute_reply":"2025-05-24T05:35:26.640371Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:14:39.208546Z","iopub.execute_input":"2025-05-24T05:14:39.208870Z","iopub.status.idle":"2025-05-24T05:14:55.585725Z","shell.execute_reply.started":"2025-05-24T05:14:39.208845Z","shell.execute_reply":"2025-05-24T05:14:55.584995Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# !zip -r /kaggle/working/tokenizer.zip /kaggle/working/tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:36:31.802324Z","iopub.execute_input":"2025-05-24T05:36:31.802657Z","iopub.status.idle":"2025-05-24T05:36:32.337407Z","shell.execute_reply.started":"2025-05-24T05:36:31.802633Z","shell.execute_reply":"2025-05-24T05:36:32.336649Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/tokenizer/ (stored 0%)\n  adding: kaggle/working/tokenizer/special_tokens_map.json (deflated 74%)\n  adding: kaggle/working/tokenizer/tokenizer.json (deflated 85%)\n  adding: kaggle/working/tokenizer/tokenizer.model (deflated 55%)\n  adding: kaggle/working/tokenizer/tokenizer_config.json (deflated 66%)\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"## Push to hugging face","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import create_repo, upload_folder\nrepo_id=\"rakibulnahin/travel-chat-llama2-7b-lora-4bit-finetuned\"\ncreate_repo(repo_id, private=False)\nupload_folder(\n    repo_id=repo_id,\n    folder_path=\"/kaggle/working/output/checkpoint-75\",\n    path_in_repo=\".\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T08:39:02.843469Z","iopub.execute_input":"2025-05-24T08:39:02.843760Z","iopub.status.idle":"2025-05-24T08:39:11.580297Z","shell.execute_reply.started":"2025-05-24T08:39:02.843740Z","shell.execute_reply":"2025-05-24T08:39:11.579544Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"353a6d5a6b3c4525aec79c98a06b3e64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0fc5b5ee7f2489eb3a531c43975dcb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67e40e423e9e43018084420560956cff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e5d813b84544539f1bd793ef5e7569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3925664839141acb9014d47a33df734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d2eec1b48e4a6db95742acda310931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42849d0e1e2640db8e65170d3361f7e4"}},"metadata":{}},{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/rakibulnahin/travel-chat-llama2-7b-lora-4bit-finetuned/commit/2e00c8bed9dcb2ae9368e1e73bb3f7628880928a', commit_message='Upload folder using huggingface_hub', commit_description='', oid='2e00c8bed9dcb2ae9368e1e73bb3f7628880928a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rakibulnahin/travel-chat-llama2-7b-lora-4bit-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='rakibulnahin/travel-chat-llama2-7b-lora-4bit-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":98},{"cell_type":"markdown","source":"## Inferencing model","metadata":{}},{"cell_type":"code","source":"tokenizer2 = AutoTokenizer.from_pretrained(\"/kaggle/working/tokenizer\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:39:06.182810Z","iopub.execute_input":"2025-05-24T05:39:06.183631Z","iopub.status.idle":"2025-05-24T05:39:06.317221Z","shell.execute_reply.started":"2025-05-24T05:39:06.183607Z","shell.execute_reply":"2025-05-24T05:39:06.316648Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"model2.eval()\n\nprompt = \"make me a solo trip in a beach area\"\ninputs = tokenizer2(prompt, return_tensors=\"pt\").to(model2.device)\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=1024,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.1\n    )\n\nresponse = tokenizer2.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:49:37.430318Z","iopub.execute_input":"2025-05-24T05:49:37.430865Z","iopub.status.idle":"2025-05-24T05:50:06.053636Z","shell.execute_reply.started":"2025-05-24T05:49:37.430843Z","shell.execute_reply":"2025-05-24T05:50:06.052985Z"}},"outputs":[{"name":"stdout","text":"make me a solo trip in a beach area. февраль [].\nThe beach area I have chosen for your solo trip is Bali, Indonesia. This island is known for its beautiful beaches, rich culture, and relaxing atmosphere. Here are some suggestions for activities to do during your solo travel:\n1. Explore the local markets: Visit the traditional markets of Bali, such as Pasar Badung or Pasar Kumbasari, to experience the vibrant colors and scents of the island's unique culture. You can shop for souvenirs, try local foods, and interact with the friendly locals.\n2. Take a yoga class: Bali is famous for its yoga retreats, and there are numerous studios throughout the island where you can take a class. Not only will this help you relax, but it will also provide an opportunity to meet other travelers and make new friends.\n3. Go on a surfing adventure: Bali is known for its excellent surfing conditions, and there are many surf schools located along the coast. If you're a beginner, don't worry – there are instructors available who will teach you how to surf. Even if you're not interested in surfing, you can still enjoy the beautiful ocean views and watch the surfers from the shore.\n4. Visit temples: Bali has many beautiful temples that are worth visiting. These temples offer a glimpse into the island's rich spiritual heritage and are often peaceful and serene places to spend time alone. Some popular temples include Tanah Lot Temple, Ulun Danu Beratan Temple, and Pura Tirtha Empul.\n5. Take a cooking class: Learn how to prepare delicious Balinese dishes by taking a cooking class. This activity will not only give you the opportunity to taste different flavors, but it will also provide an insight into the island's culinary culture.\n6. Relax at a spa: After a day of exploring and learning about the island, treat yourself to a relaxing spa treatment. There are numerous spas throughout Bali that offer various massages, facials, and other treatments. Spend some time unwinding and rejuvenating while enjoying the beautiful surroundings.\n7. Explore nature reserves: Bali has several nature reserves that are home to exotic flora and fauna. Take a guided tour through these areas to learn more about the island's unique ecosystem and spot some of the local wildlife.\n8. Take a sunset walk: End your day by taking a leisurely stroll along the beachfront or a scenic trail overlooking the ocean. As the sun sets, the sky transforms into a kaleidoscope of colors, making for a truly unforgettable experience.\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"prompt = \"I want to travel to relaxing destination with affordable accomodation and relaxing sight\"\ninputs = tokenizer2(prompt, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=1024,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.1\n    )\n\nresponse = tokenizer2.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T06:05:17.463687Z","iopub.execute_input":"2025-05-24T06:05:17.464312Z","iopub.status.idle":"2025-05-24T06:05:43.505161Z","shell.execute_reply.started":"2025-05-24T06:05:17.464287Z","shell.execute_reply":"2025-05-24T06:05:43.504369Z"}},"outputs":[{"name":"stdout","text":"I want to travel to relaxing destination with affordable accomodation and relaxing sightseeing activities. hopefully you can suggest some places that fit my criteria.\nCertainly! Based on your preferences, here are some suggestions for destinations that offer affordable accommodation and relaxing sightseeing activities:\n1. Bali, Indonesia: Bali is known as the \"Island of the Gods\" for its beautiful beaches, temples, and lush green landscapes. The island offers a range of accommodation options from budget-friendly guesthouses to luxury resorts. Some popular sightseeing activities include visiting the famous Tanah Lot Temple, exploring Ubud's art galleries, and relaxing on Kuta Beach. Prices vary depending on the season and location, but generally speaking, a budget of $50-$75 per day should cover most expenses.\n2. Thailand: Thailand is another popular destination for those seeking a relaxing vacation at an affordable price. The country is home to numerous beautiful beaches, ancient temples, and delicious street food. Affordable accommodation options include hostels and budget hotels, while sightseeing activities like visiting Ayutthaya Historical Park, hiking in Chiang Mai, or exploring Bangkok's Grand Palace are all within reach. A daily budget of $40-$60 should cover most expenses.\n3. Costa Rica: Costa Rica is a nature lover's paradise, with lush rainforests, stunning beaches, and abundant wildlife. Affordable accommodation options include eco-lodges and budget hotels, while sightseeing activities such as zip-lining through the jungle, rafting down the Reventazon River, or simply relaxing on the beach are all available. A daily budget of $60-$80 should cover most expenses.\n4. Vietnam: Vietnam is a great destination for those looking for a relaxing vacation with affordable prices. From the bustling streets of Hanoi and Ho Chi Minh City to the peaceful countryside and beautiful beaches, there's something for everyone. Budget accommodation options abound, while sightseeing activities like exploring the ancient city of Hoi An, visiting the Cu Chi Tunnels near Ho Chi Minh City, or simply enjoying a meal at a local restaurant are all within reach. A daily budget of $40-$60 should cover most expenses.\nI hope these suggestions help you plan your trip! Let me know if you have any other questions or if you need further assistance. \n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"prompt = \"Make a travel plan for a exciting experience in Vietnam with friends including various activites and relaxing experience\"\ninputs = tokenizer2(prompt, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=1024,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.1\n    )\n\nresponse = tokenizer2.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:21:53.500781Z","iopub.execute_input":"2025-05-24T09:21:53.501313Z","iopub.status.idle":"2025-05-24T09:21:53.546996Z","shell.execute_reply.started":"2025-05-24T09:21:53.501288Z","shell.execute_reply":"2025-05-24T09:21:53.545789Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3884481952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m                 )\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    572\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mhidden_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0;31m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;31m# The reason is that in some cases, an error can occur that backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 21.12 MiB is free. Process 3570 has 15.87 GiB memory in use. Of the allocated memory 15.34 GiB is allocated by PyTorch, and 231.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 21.12 MiB is free. Process 3570 has 15.87 GiB memory in use. Of the allocated memory 15.34 GiB is allocated by PyTorch, and 231.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":104},{"cell_type":"code","source":"\n# CANT DO THIS CUZ PIPELINE DOESNT SUPPORT LORA + QUANTIZATION\n# from transformers import pipeline\n# pipe = pipeline(\"text-generation\", model=\"rakibulnahin/travel-chat-llama2-7b\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T07:16:48.259575Z","iopub.execute_input":"2025-05-24T07:16:48.260351Z","iopub.status.idle":"2025-05-24T07:16:48.263817Z","shell.execute_reply.started":"2025-05-24T07:16:48.260325Z","shell.execute_reply":"2025-05-24T07:16:48.262968Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}