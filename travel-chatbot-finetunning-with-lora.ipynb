{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-02T14:20:51.772505Z",
     "iopub.status.busy": "2025-05-02T14:20:51.772258Z",
     "iopub.status.idle": "2025-05-02T14:20:53.652631Z",
     "shell.execute_reply": "2025-05-02T14:20:53.651924Z",
     "shell.execute_reply.started": "2025-05-02T14:20:51.772480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:20:53.654780Z",
     "iopub.status.busy": "2025-05-02T14:20:53.654445Z",
     "iopub.status.idle": "2025-05-02T14:22:23.667354Z",
     "shell.execute_reply": "2025-05-02T14:22:23.666637Z",
     "shell.execute_reply.started": "2025-05-02T14:20:53.654760Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, bitsandbytes, accelerate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.1\n",
      "    Uninstalling transformers-4.51.1:\n",
      "      Successfully uninstalled transformers-4.51.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.3.0\n",
      "    Uninstalling accelerate-1.3.0:\n",
      "      Successfully uninstalled accelerate-1.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.6.0 bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:22:23.668679Z",
     "iopub.status.busy": "2025-05-02T14:22:23.668355Z",
     "iopub.status.idle": "2025-05-02T14:22:51.992423Z",
     "shell.execute_reply": "2025-05-02T14:22:51.991641Z",
     "shell.execute_reply.started": "2025-05-02T14:22:23.668656Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 14:22:36.087278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746195756.326580      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746195756.391046      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"\"\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(hf_token)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType, LoraModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:22:51.993874Z",
     "iopub.status.busy": "2025-05-02T14:22:51.993288Z",
     "iopub.status.idle": "2025-05-02T14:22:54.216858Z",
     "shell.execute_reply": "2025-05-02T14:22:54.216024Z",
     "shell.execute_reply.started": "2025-05-02T14:22:51.993847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c492b11faa248af8be58ca4455d806f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "input-response1-response2.csv:   0%|          | 0.00/20.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e7a7d6c30445458be72507c6fe3d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (6009,), output: (6009,)\n"
     ]
    }
   ],
   "source": [
    "ds1 = load_dataset(\"JasleenSingh91/travel-questions-response\")\n",
    "df1 = pd.DataFrame(ds1['train'])\n",
    "input1 = df1[\"Input\"]\n",
    "output1 = df1[\"Response_2\"]\n",
    "print(f\"input: {input1.shape}, output: {output1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:22:54.218258Z",
     "iopub.status.busy": "2025-05-02T14:22:54.217734Z",
     "iopub.status.idle": "2025-05-02T14:22:55.562909Z",
     "shell.execute_reply": "2025-05-02T14:22:55.562332Z",
     "shell.execute_reply.started": "2025-05-02T14:22:54.218238Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e859aad734cb4aa8bab4b25c12b92a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/398 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c241a906c284d6b97e4fe8e2db1e7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-d5eea51e61dddc73.parquet:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f4448e4d024bc38703640d0e06f343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (110,), output: (110,)\n"
     ]
    }
   ],
   "source": [
    "ds2 = load_dataset(\"Binaryy/travel_sample_extended\")\n",
    "df2 = pd.DataFrame(ds2['train'])\n",
    "input2 = df2[\"query\"]\n",
    "output2 = df2[\"response\"]\n",
    "print(f\"input: {input2.shape}, output: {output2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:22:55.564992Z",
     "iopub.status.busy": "2025-05-02T14:22:55.564792Z",
     "iopub.status.idle": "2025-05-02T14:22:55.573542Z",
     "shell.execute_reply": "2025-05-02T14:22:55.572998Z",
     "shell.execute_reply.started": "2025-05-02T14:22:55.564977Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "res1 = '''\n",
    "Switzerland boasts a plethora of breathtaking destinations, catering to diverse interests. Here are some of the best places to visit:\n",
    "\n",
    "**Cities & Towns:**\n",
    "\n",
    "* **Zurich:** Switzerland's largest city, a global hub with a blend of modern and historic charm. Explore the Old Town (Altstadt) with its winding lanes, visit numerous museums and art galleries, enjoy a boat trip on Lake Zurich, and experience the lively nightlife.\n",
    "* **Bern:** The capital city, a UNESCO World Heritage site, known for its well-preserved medieval Old Town with charming arcades, historic fountains, and the Zytglogge clock tower. Don't miss the Einstein House and the Rose Garden for panoramic views.\n",
    "* **Lucerne:** A picturesque city on Lake Lucerne, surrounded by mountains. Admire the iconic Chapel Bridge, the Lion Monument, and enjoy the stunning views from Mount Pilatus or Mount Rigi. It's a perfect base for exploring Central Switzerland.\n",
    "* **Interlaken:** Situated between Lake Thun and Lake Brienz, it's known as the \"Adventure Capital\" offering activities like paragliding, canyoning, and hiking. It provides stunning views of the Jungfrau massif.\n",
    "* **Geneva:** A cosmopolitan city on Lake Geneva, home to international organizations like the United Nations. Visit the Jet d'Eau fountain, explore the Old Town, and enjoy the lakeside promenades. It's also a gateway to the Lavaux vineyards.\n",
    "* **Zermatt:** A car-free village nestled at the foot of the Matterhorn. It's a world-renowned destination for skiing and mountaineering, offering breathtaking views of the iconic peak. Take the Gornergrat Railway for unparalleled vistas.\n",
    "* **Lausanne:** An elegant university city on the shores of Lake Geneva, offering beautiful lake views and a rich cultural heritage. Visit the Lausanne Cathedral and the Olympic Museum.\n",
    "* **Lugano:** Located in the Italian-speaking part of Switzerland, it offers a Mediterranean vibe with a mix of Swiss and Italian culture. Enjoy boat rides on Lake Lugano and explore the charming Parco Ciani.\n",
    "* **Montreux:** A charming town on Lake Geneva, famous for its mild climate and the Freddie Mercury statue. Visit the stunning Chillon Castle and enjoy the views of the terraced vineyards of Lavaux.\n",
    "* **Grindelwald:** A picturesque alpine village in the Bernese Alps, offering stunning scenery and outdoor adventures. It's a great base for exploring the Jungfrau region.\n",
    "* **Lauterbrunnen:** A stunning valley with 72 waterfalls, surrounded by towering cliffs. The Staubbach Falls are particularly impressive. It's a gateway to the Jungfrau region and car-free villages like Wengen and Mürren.\n",
    "* **Stein am Rhein:** A beautifully preserved medieval town on the Rhine River, known for its half-timbered buildings with colorful painted facades.\n",
    "\n",
    "**Natural Wonders & Attractions:**\n",
    "\n",
    "* **The Matterhorn:** An iconic pyramid-shaped mountain in the Alps, a must-see landmark.\n",
    "* **Jungfraujoch - Top of Europe:** Home to the highest railway station in Europe, offering breathtaking views of glaciers and the surrounding peaks.\n",
    "* **Lake Geneva (Lac Léman):** Switzerland's largest lake, offering stunning views, lakeside towns, and vineyards.\n",
    "* **Lake Lucerne (Vierwaldstättersee):** A beautiful lake surrounded by mountains, offering scenic boat trips and access to various mountain excursions.\n",
    "* **Rhine Falls (Rheinfall):** One of Europe's largest waterfalls, a powerful and impressive natural spectacle.\n",
    "* **Aletsch Glacier:** The largest glacier in the Alps, a UNESCO World Heritage site.\n",
    "* **Swiss National Park:** The oldest and largest nature reserve in Switzerland, offering opportunities for wildlife observation and hiking.\n",
    "* **Oeschinensee:** A stunning mountain lake near Kandersteg, accessible by gondola.\n",
    "* **Creux du Van:** A natural rock arena in the Jura Mountains, offering panoramic views.\n",
    "* **Staubbach Falls:** One of the highest free-falling waterfalls in Europe, located in the Lauterbrunnen Valley.\n",
    "* **Trümmelbach Falls:** A series of ten glacier-fed waterfalls inside a mountain in the Lauterbrunnen Valley.\n",
    "\n",
    "**Scenic Train Rides:**\n",
    "\n",
    "* **Glacier Express:** A panoramic train journey connecting Zermatt and St. Moritz, traversing stunning alpine landscapes.\n",
    "* **Bernina Express:** A spectacular train ride through the Alps, connecting Chur and Tirano, passing over impressive viaducts and through tunnels.\n",
    "* **GoldenPass Line:** A scenic route connecting Lucerne and Montreux, offering diverse landscapes from lakes to mountains.\n",
    "* **Gotthard Panorama Express:** A journey combining a boat trip on Lake Lucerne and a train ride over the historic Gotthard railway line.\n",
    "\n",
    "When planning your trip, consider your interests (hiking, skiing, culture, relaxation), the time of year, and your budget to tailor your itinerary to the best places for you.\n",
    "\n",
    "'''\n",
    "\n",
    "res2 = '''\n",
    "Certainly I can for a better suggestion. Can you give me some personal details such as are do you have place in mind? Are you going alone or with friends, family, partner.\n",
    "Do you plan to stay in a hotel?\n",
    "What kind of budget do have in mind?\n",
    "Are you planning to travel with a car or public transport.\n",
    "What weather conditions do you prefer?\n",
    "For example, this is winter time so, so if you are planning a cozy winter plan. You can go to Switzerland it has some amazing places to visit.\n",
    "It has largest lake, offering stunning views, lakeside towns, and vineyards.\n",
    "You can Skii on the icy mountains and enjoy stunning mountanious view from the top of the mountain.\n",
    "You can ride cable cars.\n",
    "Please let me know so i can plan my trip more specifically.\n",
    "'''\n",
    "\n",
    "res3 = '''\n",
    "Considering your preferences for a warm tropical country with excellent summer weather, beautiful beaches with clean water, stunning green mountains, luxurious ocean-view accommodations, abundant activities, and great food, I highly recommend a trip to Maui, Hawaii, USA.\n",
    "\n",
    "Here's a potential 7-day travel plan for you and your wife, keeping in mind that \"summer\" in the Northern Hemisphere (where Hawaii is located) typically runs from June to August, offering warm and sunny weather:\n",
    "\n",
    "Destination: Maui, Hawaii, USA\n",
    "\n",
    "Duration: 7 Days / 6 Nights\n",
    "\n",
    "Best Time to Visit (for your preference): June to August (warmest and driest weather)\n",
    "\n",
    "Accommodation: The Ritz-Carlton, Kapalua (renowned for its luxurious ocean-view rooms, stunning location, and exceptional service) or the Four Seasons Resort Maui at Wailea (another top-tier option with breathtaking views and amenities).\n",
    "\n",
    "Day 1: Arrival in Maui & Settling into Paradise\n",
    "\n",
    "Fly into Kahului Airport (OGG) on Maui.\n",
    "Upon arrival, you'll be greeted with the fragrant scent of tropical flowers – a classic Hawaiian welcome.\n",
    "Private transfer to your chosen luxurious hotel in Kapalua or Wailea. Both locations offer stunning ocean views and upscale amenities.\n",
    "Check in and take some time to settle into your opulent ocean-view suite. Enjoy the view and the hotel's facilities.\n",
    "Evening: Enjoy a romantic welcome dinner at Merriman's Kapalua or Spago Maui, both known for their exquisite Hawaiian-inspired cuisine with oceanfront settings.\n",
    "Day 2: Exploring West Maui's Beauty\n",
    "\n",
    "Morning: After a leisurely breakfast, embark on a scenic drive along the West Maui coastline.\n",
    "Visit Kaanapali Beach, famous for its golden sands and clear turquoise waters. Relax on the beach, go for a swim, or try stand-up paddleboarding.\n",
    "Afternoon: Explore Lahaina Town, a historic whaling village with art galleries, boutiques, and waterfront restaurants.\n",
    "Consider a whale watching tour (if visiting between November and May, though the water clarity is excellent year-round).\n",
    "Evening: Witness a breathtaking sunset from the cliffs of Kapalua Point, followed by dinner at Star Noodle, a local favorite with Asian-inspired dishes.\n",
    "Day 3: The Majestic Road to Hana\n",
    "\n",
    "Full Day: Embark on the iconic Road to Hana, a scenic drive with lush rainforests, cascading waterfalls, dramatic coastal cliffs, and stunning ocean vistas.\n",
    "Stop at various points of interest along the way, such as:\n",
    "Twin Falls: An easily accessible waterfall for a refreshing dip.\n",
    "Garden of Eden Arboretum: A beautiful botanical garden showcasing Hawaiian flora.\n",
    "Waianapanapa State Park: Known for its black sand beach, sea caves, and blowholes.\n",
    "Hamoa Beach: A picturesque crescent-shaped beach.\n",
    "Pools of Ohe'o (Seven Sacred Pools): A series of cascading waterfalls and pools (check for accessibility as it can sometimes be closed due to weather).\n",
    "Pack a picnic lunch to enjoy amidst the natural beauty or dine at a local eatery in Hana.\n",
    "Evening: Return to your hotel, perhaps indulging in a relaxing couples massage.\n",
    "Day 4: Underwater Adventures & Relaxation\n",
    "\n",
    "Morning: Take a snorkeling or scuba diving trip to the Molokini Crater, a partially submerged volcanic crater teeming with marine life and crystal-clear waters. Numerous tour operators offer excursions from Ma'alaea Harbor.\n",
    "Afternoon: Return to your hotel and enjoy the luxurious amenities. Relax by the pool, unwind on the beach, or indulge in spa treatments.\n",
    "Evening: Enjoy a fine dining experience at Mama's Fish House, a legendary restaurant known for its fresh, locally sourced seafood and Polynesian ambiance (reservations highly recommended well in advance).\n",
    "Day 5: Exploring Upcountry Maui & Volcanic Wonders\n",
    "\n",
    "Morning: Drive upcountry to explore the cooler, greener side of Maui.\n",
    "Visit the Maui Tropical Plantation, a historic agricultural estate with shops, restaurants, and ziplining opportunities.\n",
    "Continue to Kula, known for its flower farms, botanical gardens, and panoramic views of the island.\n",
    "Afternoon: Drive to the summit of Haleakala National Park, a dormant volcano offering breathtaking views of the lunar-like landscape. Consider arriving in time for the sunset (reservations often required).\n",
    "Evening: Enjoy dinner at a restaurant in Kula with stunning views or head back down to the coast for more dining options.\n",
    "Day 6: Water Sports & Island Delights\n",
    "\n",
    "Morning: Indulge in some water sports activities. Consider:\n",
    "Surfing lessons: Maui has some excellent beginner-friendly surf spots.\n",
    "Kayaking or outrigger canoeing: Explore the coastline at your own pace.\n",
    "Sailing charter: Enjoy a leisurely sail along the coast.\n",
    "Afternoon: Explore the charming town of Paia, known for its eclectic shops, art galleries, and laid-back vibe. Enjoy lunch at one of its many cafes.\n",
    "Evening: Experience a traditional Hawaiian luau with Polynesian food, music, and dance for a cultural immersion. The Old Lahaina Luau is highly recommended.\n",
    "Day 7: Farewell to Paradise\n",
    "\n",
    "Morning: Enjoy a final leisurely breakfast with stunning ocean views.\n",
    "Depending on your flight schedule, you might have time for some last-minute souvenir shopping or a final dip in the ocean or pool.\n",
    "Private transfer to Kahului Airport (OGG) for your departure, filled with unforgettable memories of your luxurious and adventurous tropical getaway.\n",
    "Food Options:\n",
    "\n",
    "Maui offers a diverse culinary scene:\n",
    "\n",
    "Fresh Seafood: From upscale restaurants like Mama's Fish House to casual beachfront grills.\n",
    "Hawaiian Regional Cuisine: A fusion of local ingredients and international influences.\n",
    "Farm-to-Table: Many restaurants emphasize fresh, locally sourced produce.\n",
    "Food Trucks: Offering a variety of delicious and affordable options.\n",
    "Local Specialties: Don't miss trying poi, laulau, kalua pig, and shave ice.\n",
    "Activities:\n",
    "\n",
    "Beyond the itinerary, consider these activities:\n",
    "\n",
    "Ziplining: Experience the thrill of soaring through the rainforest canopy.\n",
    "Hiking: Maui offers trails for all levels, from coastal walks to challenging mountain treks.\n",
    "Golfing: World-class golf courses with stunning ocean views.\n",
    "Horseback Riding: Explore the island's interior on horseback.\n",
    "Helicopter Tours: Get a bird's-eye view of Maui's dramatic landscapes.\n",
    "Art Galleries and Museums: Explore the local culture and art scene.\n",
    "This itinerary is a suggestion and can be customized to your interests and pace. Maui offers a perfect blend of natural beauty, luxurious accommodations, exciting activities, and delicious food, making it an ideal destination for a memorable tropical getaway for you and your wife. Enjoy your trip!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:22:55.574750Z",
     "iopub.status.busy": "2025-05-02T14:22:55.574423Z",
     "iopub.status.idle": "2025-05-02T14:22:55.594945Z",
     "shell.execute_reply": "2025-05-02T14:22:55.594123Z",
     "shell.execute_reply.started": "2025-05-02T14:22:55.574725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({\n",
    "    \"Input\": [\"What are the best places to visit in Switzerland?\", \"Can you plan a day trip for me?\",\n",
    "              \"Make a travel plan for me and my wife on a warm tropical country with good summer weather. I want a place with nice beaches and clean water also beautiful green mountain. I have no budget problem but I want a nice view hotel with luxurious room with nice ocean view. Also there are lots of activity to do in my trip and great food options\"],\n",
    "    \"Response\": [res1, res2, res3]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:22:55.595953Z",
     "iopub.status.busy": "2025-05-02T14:22:55.595694Z",
     "iopub.status.idle": "2025-05-02T14:22:55.614803Z",
     "shell.execute_reply": "2025-05-02T14:22:55.614135Z",
     "shell.execute_reply.started": "2025-05-02T14:22:55.595930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_input = pd.concat([input1, input2, df3[\"Input\"]])\n",
    "all_input = all_input.reset_index(drop=True)\n",
    "all_output = pd.concat([output1, output2, df3[\"Response\"]])\n",
    "all_output = all_output.reset_index(drop=True)\n",
    "\n",
    "# all_input = pd.concat([input2, df3[\"Input\"]])\n",
    "# all_input = all_input.reset_index(drop=True)\n",
    "# all_output = pd.concat([output2, df3[\"Response\"]])\n",
    "# all_output = all_output.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:41:19.307739Z",
     "iopub.status.busy": "2025-05-02T14:41:19.307020Z",
     "iopub.status.idle": "2025-05-02T14:41:22.636297Z",
     "shell.execute_reply": "2025-05-02T14:41:22.635746Z",
     "shell.execute_reply.started": "2025-05-02T14:41:19.307714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "q_config = BitsAndBytesConfig(load_in_4bit=True) \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,\n",
    "    # quantization_config = q_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    # device_map=\"auto\"\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:23:09.160805Z",
     "iopub.status.busy": "2025-05-02T14:23:09.160218Z",
     "iopub.status.idle": "2025-05-02T14:23:30.276590Z",
     "shell.execute_reply": "2025-05-02T14:23:30.275750Z",
     "shell.execute_reply.started": "2025-05-02T14:23:09.160786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1079f3303584b3ca85adfcf0f568f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5509 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a43cdb54464da6a5c459897457ec64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5509 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prompt(example):\n",
    "    prompt = f\"\"\"\n",
    "    <|user|> {example[\"input\"]}\n",
    "\n",
    "    <|bot|> {example[\"output\"]}\n",
    "    \n",
    "    \"\"\"\n",
    "     \n",
    "    p_input = tokenizer(prompt, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    p_input[\"labels\"] = p_input[\"input_ids\"].copy()\n",
    "    return p_input\n",
    "\n",
    "ds = Dataset.from_pandas(pd.DataFrame({\n",
    "    \"input\": all_input,\n",
    "    \"output\": all_output\n",
    "}))\n",
    "\n",
    "split_dataset = ds.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "train_dataset = train_dataset.map(prompt)\n",
    "eval_dataset = train_dataset.map(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:41:28.615062Z",
     "iopub.status.busy": "2025-05-02T14:41:28.614783Z",
     "iopub.status.idle": "2025-05-02T14:41:28.713913Z",
     "shell.execute_reply": "2025-05-02T14:41:28.713288Z",
     "shell.execute_reply.started": "2025-05-02T14:41:28.615042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.02,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"o_proj\"],\n",
    "    use_rslora = True\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model ,lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:41:42.139883Z",
     "iopub.status.busy": "2025-05-02T14:41:42.139288Z",
     "iopub.status.idle": "2025-05-02T14:41:42.195879Z",
     "shell.execute_reply": "2025-05-02T14:41:42.195082Z",
     "shell.execute_reply.started": "2025-05-02T14:41:42.139859Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2224886751.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/results/lora\",\n",
    "    eval_strategy = \"steps\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 8,  # micro batching\n",
    "    gradient_accumulation_steps = 8,\n",
    "    logging_dir=\"./logs\",  # Logging directory\n",
    "    save_steps = 100,  # How often to save the model\n",
    "    fp16=True,  # Use mixed precision training\n",
    "    # device = torch.device(\"cuda\")\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:39:19.502942Z",
     "iopub.status.busy": "2025-05-02T14:39:19.502113Z",
     "iopub.status.idle": "2025-05-02T14:39:19.506742Z",
     "shell.execute_reply": "2025-05-02T14:39:19.506036Z",
     "shell.execute_reply.started": "2025-05-02T14:39:19.502916Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T14:41:48.935376Z",
     "iopub.status.busy": "2025-05-02T14:41:48.934563Z",
     "iopub.status.idle": "2025-05-02T14:48:24.629872Z",
     "shell.execute_reply": "2025-05-02T14:48:24.628624Z",
     "shell.execute_reply.started": "2025-05-02T14:41:48.935333Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/129 02:38 < 5:32:06, 0.01 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3780\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3782\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3784\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2448\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-02T14:23:37.588600Z",
     "iopub.status.idle": "2025-05-02T14:23:37.588964Z",
     "shell.execute_reply": "2025-05-02T14:23:37.588803Z",
     "shell.execute_reply.started": "2025-05-02T14:23:37.588788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-02T14:23:37.590481Z",
     "iopub.status.idle": "2025-05-02T14:23:37.590919Z",
     "shell.execute_reply": "2025-05-02T14:23:37.590719Z",
     "shell.execute_reply.started": "2025-05-02T14:23:37.590701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-02T14:23:37.592203Z",
     "iopub.status.idle": "2025-05-02T14:23:37.592532Z",
     "shell.execute_reply": "2025-05-02T14:23:37.592345Z",
     "shell.execute_reply.started": "2025-05-02T14:23:37.592335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"/kaggle/working/fine_tuned_llama_qlora\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/fine_tuned_llama_qlora\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
